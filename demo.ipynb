{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa0cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/iccvsubmission10189/iccvsubmission10189_supplementary/archive/refs/heads/main.zip\n",
    "!unzip main.zip\n",
    "!mv iccvsubmission10189_supplementary-main/*.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6369b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from model import DSPDH_temporal_future\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "import tqdm\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "import io\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from utils import to_colormap, human_chains_ixs, chains_ixs, get_chains, get_human_chains, subplot_bones, JOINTS_NAMES, unravel_indices\n",
    "import cv2\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from matplotlib import gridspec\n",
    "plt.rcParams['animation.html'] = 'jshtml'\n",
    "\n",
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# set device and workers\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b35a2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_skeleton(type, joints, color, figname, viewangle):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    if type == 'human':\n",
    "        ax.set_xlim(-1.2, 1.2)\n",
    "        ax.set_ylim(1.8, 3.7)\n",
    "        ax.set_zlim(-1.1, 1.1)\n",
    "    else:\n",
    "        ax.set_xlim(-0.65, 0.65)\n",
    "        ax.set_ylim(1.8, 2.3)\n",
    "        ax.set_zlim(-0.3, 0.5)\n",
    "\n",
    "    if type == 'human':\n",
    "        chains = get_human_chains(joints, *human_chains_ixs)\n",
    "    else:\n",
    "        chains = get_chains(joints, *chains_ixs)\n",
    "    ax.scatter3D(joints[:, 0], joints[:, 2], joints[:, 1], c=color, depthshade=True)\n",
    "    subplot_bones(chains, ax, c=color)\n",
    "\n",
    "    ax.get_xaxis().set_ticklabels([])\n",
    "    ax.get_yaxis().set_ticklabels([])\n",
    "    ax.set_zticklabels([])\n",
    "    ax.grid(b=None)\n",
    "    ax.view_init(10, viewangle)\n",
    "    fig.canvas.draw()\n",
    "    # grab the pixel buffer and dump it into a numpy array\n",
    "    im = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "    plt.close(fig)\n",
    "    #plt.savefig(figname, dpi=500, bbox_inches=\"tight\")\n",
    "    #plt.cla()\n",
    "    return im[:,:,:3]\n",
    "    #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9f9f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download pretrained model and data\n",
    "!pip install gdown\n",
    "!mkdir data\n",
    "\n",
    "import gdown\n",
    "output = './data/'\n",
    "\n",
    "\n",
    "gdrive_urls = [\n",
    "    'https://drive.google.com/file/d/1JdWVNfT-IrAS0GbSeZYZswmI1NS7ur0W/view?usp=share_link',\n",
    "    'https://drive.google.com/file/d/1sgKQ9QnUaUhHFDD31QHlsbxaPfNynE9j/view?usp=share_link',\n",
    "    'https://drive.google.com/file/d/1xURG1YpOWCp33283P3WzFySXIACQG1Rv/view?usp=share_link',\n",
    "    'https://drive.google.com/file/d/1B3jgMlK0MAsrkOQS8qPghQ5FGSNtqyyj/view?usp=share_link',\n",
    "    'https://drive.google.com/file/d/1BTge-TKqAVHRGqFz92vSWTu6kUzWUaWJ/view?usp=share_link',\n",
    "    'https://drive.google.com/file/d/1Bjbd60BGhEjb4aYjVtNPXnSkJndjTXkS/view?usp=share_link',\n",
    "    'https://drive.google.com/file/d/1U4tShglCG8rhC38RJNIAmt-Q56UjLf5P/view?usp=share_link',\n",
    "    'https://drive.google.com/file/d/15dSkQzutMvtXror-tekFUI41HS6JIWkG/view?usp=share_link'\n",
    "]\n",
    "\n",
    "urls = [u.replace('/file/d/','/uc?id=').replace('/view?usp=share_link','') for u in gdrive_urls]\n",
    "\n",
    "for n, u in enumerate(urls):\n",
    "    print(f'downloading file {n+1}/{len(urls)}...')\n",
    "    gdown.download(u + \"&confirm=t\", output, quiet=False)",
    "\n",
    "gdown.download('https://www.dropbox.com/s/fao4vfui5ta871c/model_pretrained.pth?dl=1', output + '/model_pretrained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d161272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load intrinsic parameters and pretrained models\n",
    "input_K = torch.Tensor(np.load('data/intrinsic.npy')).to(device)\n",
    "resume = 'data/model_pretrained.pth'\n",
    "model = DSPDH_temporal_future(c=32, joints_num=16, deltas=False, future_window_size=4)\n",
    "model = model.to(device)\n",
    "weights_dir = Path(resume)\n",
    "checkpoint = torch.load(str(weights_dir), map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model\"], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da7c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the example:\n",
    "#center_cam_left_arm.pkl   left_cam_left_arm.pkl   right_cam_left_arm.pkl\n",
    "#center_cam_right_arm.pkl  left_cam_right_arm.pkl  right_cam_right_arm.pkl\n",
    "data = pickle.load(open(\"data/center_cam_left_arm.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs\n",
    "depth_image = torch.Tensor(data['xyz'])\n",
    "prev_joint = torch.Tensor(data['buffer'])\n",
    "joint_gt = data['joints_3d_gt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ab44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference of the model\n",
    "joints_pred_total = []\n",
    "heatmap_pred_total = []\n",
    "for i in tqdm.tqdm(range(len(depth_image))):\n",
    "    depth_image_current = depth_image[i].to(device)\n",
    "    prev_joint_current = prev_joint[i].to(device)\n",
    "    with torch.no_grad():\n",
    "        heatmap_pred, heatmap_pred_fut = model(depth_image_current, prev_joint_current)\n",
    "        heatmap_pred = (heatmap_pred + 1) / 2.\n",
    "        heatmap_pred_fut = (heatmap_pred_fut + 1) / 2.\n",
    "\n",
    "        real_H, real_W = heatmap_pred.size()[2:]\n",
    "\n",
    "        # compute 3D pose from UV and UZ heatmaps of current frame\n",
    "        B, C, H, W = heatmap_pred.shape\n",
    "        joints_3d_pred = torch.ones((B, C // 2, 3)).to(device)\n",
    "        max_uv = heatmap_pred[:, :C // 2].reshape(-1, real_H * real_W).argmax(1)\n",
    "        joints_3d_pred[..., :2] = unravel_indices(max_uv, (real_H, real_W)).view(B, C // 2, -1)\n",
    "        joints_3d_pred[..., [0, 1]] = joints_3d_pred[..., [1, 0]]\n",
    "        # add Z coordinate from UZ heatmap\n",
    "        max_uz = heatmap_pred[:, C // 2:].reshape(-1, real_H * real_W).argmax(1)\n",
    "        z = unravel_indices(max_uz, (real_H, real_W)).view(B, C // 2, -1)[..., 0:1]\n",
    "        Z_min, _, dZ = [500, 3380, 15]\n",
    "        z = ((z * dZ) + Z_min) / 1000\n",
    "        # convert 2D predicted joints to 3D coordinate multiplying by inverse intrinsic matrix\n",
    "        inv_intrinsic = torch.inverse(input_K).unsqueeze(1).repeat(1, joints_3d_pred.shape[1], 1, 1)\n",
    "        joints_3d_pred = (inv_intrinsic @ joints_3d_pred[..., None]).squeeze(-1)\n",
    "        joints_3d_pred *= z\n",
    "        joints_3d_pred[..., 1] *= -1  # invert Y axis for left-handed reference frame\n",
    "        joints_pred = joints_3d_pred.cpu().numpy()\n",
    "        \n",
    "        joints_pred_total.append(joints_pred)\n",
    "        heatmap_pred_total.append(heatmap_pred.cpu().numpy())\n",
    "joints_pred_total = np.concatenate(joints_pred_total)\n",
    "heatmap_pred_total = np.concatenate(heatmap_pred_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eee4d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "depth = data['depth']\n",
    "heat_uv_total = []\n",
    "heat_uz_total = []\n",
    "joints_total = []\n",
    "heatmap_uv_pred = heatmap_pred_total[:, :16]\n",
    "heatmap_uz_pred = heatmap_pred_total[:, 16:]\n",
    "for i in tqdm.tqdm(range(len(heatmap_uv_pred))):\n",
    "    heat_uv_total.append(to_colormap(heatmap_uv_pred[i][None, ...])[0].transpose(1, 2, 0))\n",
    "    heat_uz_total.append(to_colormap(heatmap_uz_pred[i][None, ...])[0].transpose(1, 2, 0))\n",
    "    joints_total.append(plot_skeleton('robot', joints_pred_total[i], '#27ae60', '', 280)[80:480-80, 120:640-120])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172bda86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "a = np.array(depth)\n",
    "b = np.array(heat_uv_total) * 255\n",
    "c = np.array(heat_uz_total) * 255\n",
    "d = np.array(joints_total)[:,:,:,::-1]\n",
    "\n",
    "a_pad = np.pad(a, pad_width=((0, 0), (64, 64), (8,8), (0, 0)), constant_values=(255, 255))\n",
    "b_pad = np.pad(b, pad_width=((0, 0), (64, 64), (8,8), (0, 0)), constant_values=(255, 255))\n",
    "c_pad = np.pad(c, pad_width=((0, 0), (64, 64), (8,8), (0, 0)), constant_values=(255, 255))\n",
    "\n",
    "# top = np.concatenate((a_pad,b_pad), axis=1)\n",
    "# bottom = np.concatenate((c_pad,d), axis=1)\n",
    "\n",
    "top = np.concatenate((a_pad,b_pad), axis=1)\n",
    "bottom = np.concatenate((d,c_pad), axis=1)\n",
    "\n",
    "montage = np.concatenate((top, bottom), axis=2)\n",
    "\n",
    "imgs = [Image.fromarray(np.uint8(frame)) for frame in montage]\n",
    "# duration is the number of milliseconds between frames; this is 40 frames per second\n",
    "imgs[0].save(\"visualization.gif\", save_all=True, append_images=imgs[1:], duration=50, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a507b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_file = \"visualization.gif\"\n",
    "display(widgets.HTML(f'<img src=\"{gif_file}\" width=\"750\" align=\"center\">'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c0f42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda18459",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c55b54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
